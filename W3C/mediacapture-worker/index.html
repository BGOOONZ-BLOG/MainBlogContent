<!DOCTYPE html>
<html>
  <head>
    <title>
      Media Capture Stream with Worker
    </title>
    <meta charset='utf-8'>
    <script src='https://www.w3.org/Tools/respec/respec-w3c-common' async
    class='remove'>
    </script>
    <script class='remove'>
      var respecConfig = {
          // specification status (e.g. WD, LCWD, WG-NOTE, etc.). If in doubt use ED.
          specStatus:           "ED",

          // the specification's short name, as in http://www.w3.org/TR/short-name/
          shortName:            "mediacapture-worker",

          // if your specification has a subtitle that goes below the main
          // formal title, define it here
          // subtitle   :  "an excellent document",

          // if you wish the publication date to be other than the last modification, set this
          // publishDate:  "2009-08-06",

          // if the specification's copyright date is a range of years, specify
          // the start date here:
          // copyrightStart: "2005"

          // if there is a previously published draft, uncomment this and set its YYYY-MM-DD date
          // and its maturity status
          // previousPublishDate:  "1977-03-15",
          // previousMaturity:  "WD",

          // if there a publicly available Editor's Draft, this is the link
           edDraftURI:           "https://w3c.github.io/mediacapture-worker/",

          // if this is a LCWD, uncomment and set the end of its review period
          // lcEnd: "2009-08-05",

          // editors, add as many as you like
          // only "name" is required
          editors:  [
              {
                  name:       "Chia-hung Tai"
    //              ,   url:        "http://example.org/"
              ,   mailto:     "ctai@mozilla.com"
              ,   company:    "Mozilla"
              ,   companyURL: "https://www.mozilla.org/en-US/foundation/moco/"
              },
              {
                  name:       "Robert O'Callahan"
    //              ,   url:        "http://example.org/"
              ,   company:    "Mozilla"
              ,   companyURL: "https://www.mozilla.org/en-US/foundation/moco/"
              },
              {
                  name:       "Tzuhao Kuo"
    //              ,   url:        "http://example.org/"
              ,   mailto:     "tkuo@mozilla.com"
              ,   company:    "Mozilla"
              ,   companyURL: "https://www.mozilla.org/en-US/foundation/moco/"
              },
              {
                  name:       "Anssi Kostiainen",
                  company:    "Intel",
                  companyURL: "http://www.intel.com/"
              },
          ],

          // name of the WG
          wg: [
                        "Web Real-Time Communication Working Group",
                        "Device APIs Working Group"
          ],
          // URI of the public WG page
          wgURI: [      "http://www.w3.org/2011/04/webrtc/",
                        "http://www.w3.org/2009/dap/"
          ],
          // name (without the @w3c.org) of the public mailing to which comments are due
          wgPublicList: "public-media-capture",

          // URI of the patent status for this WG, for Rec-track documents
          // !!!! IMPORTANT !!!!
          // This is important for Rec-track documents, do not copy a patent URI from a random
          // document unless you know what you're doing. If in doubt ask your friendly neighbourhood
          // Team Contact.
          wgPatentURI: [
            "http://www.w3.org/2004/01/pp-impl/47318/status",
            "http://www.w3.org/2004/01/pp-impl/43696/status"
          ],
          // !!!! IMPORTANT !!!! MAKE THE ABOVE BLINK IN YOUR HEAD
          license: "w3c-software-doc",
          otherLinks: [{
            key: "Participate",
            data: [
              {
                value: "public-media-capture@w3.org",
                href: "http://lists.w3.org/Archives/Public/public-media-capture/"
              },
              {
                value: "GitHub w3c/mediacapture-worker",
                href: "https://github.com/w3c/mediacapture-worker/"
              },
              {
                value: "GitHub w3c/mediacapture-worker/issues",
                href: "https://github.com/w3c/mediacapture-worker/issues"
              },
              {
                value: "GitHub w3c/mediacapture-worker/commits",
                href: "https://github.com/w3c/mediacapture-worker/commits/"
              }
            ]
          }]
      };
    </script>
  </head>
  <body>
    <section id='abstract'>
      <p>
        This specification defines new <a>VideoMonitor</a> and
        <a>VideoProcessor</a> interfaces, and extends <a href=
        "#mediastreamtrack-interface"><code>MediaStreamTrack</code></a> and
        <a href="#imagebitmap-extensions"><code>ImageBitmap</code></a>
        interfaces to enable processing of video frames using script in a
        performant manner.
      </p>
    </section>
    <section id='sotd'>
      <div style="border: solid 6px red; padding: 10px; color: red; font-weight: bold;">
        Work on this document has been discontinued and it should not be
        referenced or used as a basis for implementation.
      </div>
    </section>
    <section>
      <h2>
        Introduction
      </h2>
      <p>
        The HTML specification [[!HTML]] defines the <a>ImageBitmap</a>
        interface that represents a bitmap image. The Media Capture and Streams
        specification [[!GETUSERMEDIA]] allows a web page to access video
        streams sourced from cameras that are exposed to the underlying
        platform and defines the <a>MediaStreamTrack</a> interface that
        represents a media source. This specification extends these interfaces
        to define a model for processing video frames sourced from media stream
        tracks by a script in a performant manner. This enables use cases such
        as video editing, digital image processing, and object recognition
        among other advanced media usages on the Web Platform in a performant
        manner.
      </p>
      <p>
        In this model, a <a>video monitor</a> or <a>video processor</a> is
        associated with an <a>input media stream track</a>. Events containing
        <a data-lt="input frame">input frame</a> data from the <a>input media
        stream track</a> are dispatched at the <a>video monitor</a> or <a>video
        processor</a>. A Web developer is able to monitor and process the
        <a>input frame</a> and produce an <a>output frame</a> using script.
        Finally, the provided <a>output frame</a> is fed into the <a>output
        media stream track</a> on the main thread. The <a>output media stream
        track</a> can then be used to construct a media stream and, for
        example, provided to media sinks such as <code>&lt;video&gt;</code> for
        rendering.
      </p>
      <p>
        The design principle of this push-like mechanism for video processing
        is depicted in the following figure:
      </p>
      <p>
        <img alt="The relationship between Worker and MediaStreamTrack" src=
        "images/overview.png" style="width:60%">
      </p>
    </section>
    <section>
      <h2>
        Use cases and requirements
      </h2>
      <p>
        This specification attempts to address the <a href=
        "https://wiki.mozilla.org/Project_FoxEye#Use_Cases">Use Cases and
        Requirements</a> for expanding the Web Platform to support image
        processing and computer vision usages.
      </p>
    </section>
    <section>
      <h2>
        Conformance
      </h2>
      <p>
        This specification defines conformance criteria that apply to a single
        product: the <dfn>user agent</dfn> that implements the interfaces that
        it contains.
      </p>
      <p>
        Implementations that use ECMAScript to implement the APIs defined in
        this specification must implement them in a manner consistent with the
        ECMAScript Bindings defined in the Web IDL specification [[!WEBIDL]],
        as this specification uses that specification and terminology.
      </p>
      <p>
        The key words "MUST", "MUST NOT", "REQUIRED", "SHOULD", "SHOULD NOT",
        "RECOMMENDED", "MAY", and "OPTIONAL" in the normative parts of this
        document are to be interpreted as described in RFC2119. For
        readability, these words do not appear in all uppercase letters in this
        specification. [[!RFC2119]]
      </p>
    </section>
    <section>
      <h2>
        Dependencies
      </h2>
      <p>
        The <code><a href=
        "http://w3c.github.io/mediacapture-main/getusermedia.html#idl-def-MediaStreamTrack">
        <dfn>MediaStreamTrack</dfn></a></code> and <code><a href=
        "http://w3c.github.io/mediacapture-main/getusermedia.html#idl-def-MediaStream">
        <dfn>MediaStream</dfn></a></code> interfaces this specification extends
        and the <a href=
        "http://w3c.github.io/mediacapture-main/#dfn-source"><dfn>source</dfn></a>
        concept are defined in [[!GETUSERMEDIA]].
      </p>
      <p>
        The <code><a href=
        "http://www.w3.org/TR/html51/webappapis.html#imagebitmap"><dfn>ImageBitmap</dfn></a></code>
        interface and <code><a href=
        "http://www.w3.org/TR/html51/webappapis.html#imagebitmapfactories"><dfn>
        ImageBitmapFactories</dfn></a></code> interface this specification
        extends are defined in [[!HTML51]].
      </p>
      <p>
        The <code><a href=
        "http://www.w3.org/TR/WebIDL-1/#common-BufferSource"><dfn>BufferSource</dfn></a></code>
        and the <code><a href=
        "http://www.w3.org/TR/WebIDL-1/#idl-ArrayBuffer"><dfn>ArrayBuffer</dfn></a></code>
        are defined in [[!WEBIDL]]
      </p>
      <p>
        The <code><a href=
        "http://www.ecma-international.org/ecma-262/6.0/#sec-promise-objects">Promise</a></code>
        object is defined in [[!ECMASCRIPT]].
      </p>
      <p>
        The following concepts and interfaces are defined in [[!HTML]]:
      </p>
      <ul>
        <li>
          <dfn><a href=
          "https://html.spec.whatwg.org/multipage/webappapis.html#event-handlers">
          Event handler</a></dfn>
        </li>
        <li>
          <dfn><a href=
          "https://html.spec.whatwg.org/multipage/webappapis.html#event-handler-event-type">
          event handler event type</a></dfn>
        </li>
        <li>
          <dfn><a href=
          "https://html.spec.whatwg.org/multipage/webappapis.html#event-handler-idl-attributes">
          event handler IDL attribute</a></dfn>
        </li>
        <li>
          <dfn><a href=
          "https://html.spec.whatwg.org/multipage/infrastructure.html#in-parallel">
          in parallel</a></dfn>
        </li>
        <li>
          <dfn><a href=
          "https://html.spec.whatwg.org/#cropped-to-the-source-rectangle">cropped
          to the source rectangle</a></dfn>
        </li>
        <li>
          <dfn><a href=
          "https://html.spec.whatwg.org/#concept-transferable-neutered">neutered</a></dfn>
        </li>
      </ul>
    </section>
    <section>
      <h2>
        Terminology
      </h2>
      <dl>
        <dt>
          <a><dfn>Video monitor</dfn></a>
        </dt>
        <dd>
          Captures an <a>input frame</a> from an <a>input media stream
          track</a>. A <a>video monitor</a> is said to be
          <dfn>monitoring</dfn>, if the <a>video monitor</a> is associated with
          an <a>input media stream track</a>. <a data-lt=
          "video monitor event">Video monitor events</a> are dispatched at a
          <a>video monitor</a>.
        </dd>
        <dt>
          <a><dfn>Video processor</dfn></a>
        </dt>
        <dd>
          Similarly captures an <a>input frame</a> from an <a>input media
          stream track</a>, and in addition, processes the provided <a>output
          frame</a> for use as the <a>source</a> for the <a>output media stream
          track</a>. A <a>video processor</a> is said to be
          <dfn>processing</dfn>, if the <a>video processor</a> is associated
          with both an <a>input media stream track</a> and an <a>output media
          stream track</a>. <a data-lt="video processor event">Video processor
          events</a> are dispatched at a <a>video processor</a>.
        </dd>
      </dl>
      <p>
        An <dfn>input frame</dfn> is an <a>ImageBitmap</a> object representing
        a frame sourced from the <a>input media stream track</a> associated
        with <a>video monitor</a> or <a>video processor</a>.
      </p>
      <p>
        An <dfn>output frame</dfn> is an <a>ImageBitmap</a> object assigned to
        the <a>outputImageBitmap</a> attribute of the
        <a>VideoProcessorEvent</a> dispatched at <a>video processor</a>.
      </p>
      <p>
        The <dfn>input media stream track</dfn> is the <a>MediaStreamTrack</a>
        object on which the <code><a>addVideoMonitor</a>()</code> or
        <code><a>addVideoProcessor</a>()</code> method is invoked.
      </p>
      <p>
        The <dfn>output media stream track</dfn> is a <a>MediaStreamTrack</a>
        object returned by the invocation of the
        <code><a>addVideoProcessor</a>()</code> method.
      </p>
      <p>
        There are two kinds of <dfn data-lt="video event">video events</dfn>:
      </p>
      <dl>
        <dt>
          <dfn data-lt="video monitor event">Video monitor events</dfn>
        </dt>
        <dd>
          These events are represented by <a>VideoMonitorEvent</a> objects that
          are dispatched at a <a>VideoMonitor</a> and provide read access to
          <a>input frame</a> data.
        </dd>
      </dl>
      <dl>
        <dt>
          <dfn data-lt="video processor event">Video processor events</dfn>
        </dt>
        <dd>
          These events are represented by <a>VideoProcessorEvent</a> objects
          that are dispatched at a <a>VideoProcessor</a> and provide read
          access to <a>input frame</a> data and write access to <a>output
          frame</a> data.
        </dd>
      </dl>
    </section>
    <section>
      <h2>
        <a>VideoMonitor</a> and <a>VideoProcessor</a> interfaces
      </h2>
      <p>
        The <a>VideoMonitor</a> interface is used to analyze and
        <a>VideoProcessor</a> interface in addition to generate and process
        video data directly using JavaScript. The <a data-lt=
        "video monitor event">video monitor events</a> are dispatched at
        <a>VideoMonitor</a> objects, and <a data-lt=
        "video processor event">video processor events</a> are dispatched at
        <a>VideoProcessor</a> objects.
      </p>
      <pre class="idl">
          [Constructor]
          interface VideoMonitor : EventTarget {
              attribute EventHandler onvideomonitorchange;
          };
        </pre>
      <pre class="idl">
          [Constructor]
          interface VideoProcessor : VideoMonitor {
              attribute EventHandler onvideoprocessorchange;
          };
        </pre>
      <p>
        The following are the <a data-lt="event handler">event handlers</a>
        (and their corresponding <a data-lt="event handler event type">event
        handler event types</a>) that must be supported, as event handler IDL
        attributes, by objects implementing the <a>VideoMonitor</a> interface:
      </p>
      <table class="simple">
        <thead>
          <tr>
            <th>
              Event handlers
            </th>
            <th>
              Event handler event type
            </th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>
              <dfn><code>onvideomonitorchange</code></dfn>
            </td>
            <td>
              <dfn><code>videomonitorchange</code></dfn>
            </td>
          </tr>
        </tbody>
      </table>
      <p>
        The following are the <a data-lt="event handler">event handlers</a>
        (and their corresponding <a data-lt="event handler event type">event
        handler event types</a>) that must be supported, as event handler IDL
        attributes, by objects implementing the <a>VideoProcessor</a>
        interface:
      </p>
      <table class="simple">
        <thead>
          <tr>
            <th>
              Event handlers
            </th>
            <th>
              Event handler event type
            </th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>
              <dfn><code>onvideoprocessorchange</code></dfn>
            </td>
            <td>
              <dfn><code>videoprocessorchange</code></dfn>
            </td>
          </tr>
        </tbody>
      </table>
      <section>
        <h2>
          Video event firing
        </h2>
        <p>
          To <dfn>fire a video event</dfn> named <var>e</var>, the user agent
          must run the following steps:
        </p>
        <ol>
          <li>If <var>e</var> is <a>videomonitorchange</a>, create a
          <a>VideoMonitorEvent</a>, initialize it to have a given name
          <var>e</var>, to not bubble, to not be cancelable.
          </li>
          <li>If <var>e</var> is <a>videoprocessorchange</a>, create a
          <a>VideoProcessorEvent</a>, initialize it to have a given name <var>
            e</var>, to not bubble, to not be cancelable.
          </li>
          <li>Initialize the <a>trackId</a> attribute to the value of the
          <code>id</code> attribute of the <a>input media stream track</a>.
          </li><!-- TODO -->
          <li>Initialize the <a>playbackTime</a> attribute to the value of the
          <code>currentTime</code> attribute of the <a>MediaStream</a> that
          contains the <a>input media stream track</a>.
          </li>
          <li>Initialize the <a>inputImageBitmap</a> attribute to the bitmap
          data of the <a>input media stream track</a>'s video frame at the
          current stream position at <a>playbackTime</a>.
          </li>
          <li>If <var>e</var> is <a>videomonitorchange</a>, dispatch the newly
          created <a>VideoMonitorEvent</a> object at each <a>video monitor</a>
          that is <a>monitoring</a>.
          </li>
          <li>If <var>e</var> is <a>videoprocessorchange</a>, initialize the
          <a>outputImageBitmap</a> attribute to null, and dispatch the newly
          created <a>VideoProcessorEvent</a> object at each <var>target</var>.
          </li>
        </ol>
      </section>
    </section>
    <section>
      <h2>
        <code>VideoMonitorEvent</code> interface
      </h2>
      <p>
        The <a>video monitor event</a> contains an <a>input frame</a> and its
        metadata originating from the <a>input media stream track</a>. It uses
        the <a>VideoMonitorEvent</a> interface for its
        <a>videomonitorchange</a> events:
      </p>
      <pre class="idl">
          [Constructor(DOMString type, optional VideoMonitorEventInit videoMonitorEventInitDict)]
          interface VideoMonitorEvent : Event {
              readonly    attribute DOMString   trackId;
              readonly    attribute double      playbackTime;
              readonly    attribute ImageBitmap? inputImageBitmap;
          };
          dictionary VideoMonitorEventInit : EventInit {
              required DOMString    trackId;
              required double       playbackTime;
              required ImageBitmap? inputImageBitmap;
          };
        </pre>
      <div>
        <p>
          The <dfn><code>trackId</code></dfn> attribute must return the value
          it was initialized to. When the object is created, this attribute
          must be initialized to the empty string. It represents the identifier
          that is shared with the <a>video monitor</a> and its <a>input media
          stream track</a>.
        </p>
        <p>
          <!-- TODO: is this current stream position? -->
           The <dfn><code>playbackTime</code></dfn> attribute must return the
          value it was initialized to. When the object is created, this
          attribute must be initialized to zero. It represents the current
          stream position, in seconds.
        </p>
        <p>
          The <dfn><code>inputImageBitmap</code></dfn> attribute must return
          the value it was initialized to. When the object is created, this
          attribute must be initialized to null. It represents the
          <a>ImageBitmap</a> object whose bitmap data is provided by the
          <a>input media stream track</a>.
        </p>
      </div>
      <p>
        When a user agent is required to <dfn>fire a video monitor event</dfn>,
        it must <a>fire a video event</a> named <a>videomonitor</a>.
      </p>
    </section>
    <section>
      <h2>
        <code>VideoProcessorEvent</code> interface
      </h2>
      <p>
        The <a>video processor event</a> inherits from the <a>video monitor
        event</a>, and in addition provides means to programmatically construct
        an <a>output frame</a> for the <a>output media stream track</a>. It
        uses the <a>VideoProcessorEvent</a> interface for its
        <a>videoprocessorchange</a> events:
      </p>
      <pre class="idl">
          [Constructor(DOMString type, optional VideoProcessorEventInit videoProcessorEventInitDict)]
          interface VideoProcessorEvent : VideoMonitorEvent {
                          attribute ImageBitmap? outputImageBitmap;
          };

          dictionary VideoProcessorEventInit : VideoMonitorEventInit {
              required ImageBitmap? outputImageBitmap;
          };
        </pre>
      <div>
        <p>
          The <dfn><code>outputImageBitmap</code></dfn> attribute must return
          the value it was initialized to. When the object is created, this
          attribute must be initialized to null. It represents the
          <a>ImageBitmap</a> object which on setting, must cause the user agent
          to run the following steps:
        </p>
      </div>
      <ol>
        <li>Let <var>output bitmap</var> be the <a>ImageBitmap</a> object
        assigned to the <a>outputImageBitmap</a> attribute.
        </li>
        <li>Let <var>output media stream track</var> be the
        <a>MediaStreamTrack</a> returned by the
        <code><a>addVideoProcessor</a>()</code> method.
        </li>
        <li>
          <!-- TODO: this should be defined better. -->
          Set <var>output bitmap</var> as the <a>source</a> of the <var>output
          media stream track</var>.
        </li>
      </ol>
      <p>
        When a user agent is required to <dfn>fire a video processor
        event</dfn>, it must <a>fire a video event</a> named
        <a>videoprocessorchange</a>.
      </p>
      <div class="note">
        <p>
          Ideally the <code>MediaStreamTrack</code> should dispatch each video
          frame through <a>VideoProcessorEvent</a>. But sometimes the worker
          thread could not process the frame in time. So the implementation
          could skip the frame to avoid high memory footprint. In such case, we
          might not be able to process every frame in a real time
          <code>MediaStream</code>.
        </p>
      </div>
    </section>
    <section>
      <h2>
        MediaStreamTrack extensions
      </h2>
      <section>
        <h2>
          <code>MediaStreamTrack</code> interface
        </h2>
        <pre class="idl">
          partial interface MediaStreamTrack {
              void              addVideoMonitor(VideoMonitor monitor);
              void              removeVideoMonitor(VideoMonitor monitor);
              MediaStreamTrack  addVideoProcessor(VideoProcessor processor);
              void              removeVideoProcessor();
          };
        </pre>
        <div>
          <p>
            The <code><dfn>addVideoMonitor</dfn>()</code> method, when invoked,
            must run these steps:
          </p>
          <ol>
            <li>Let <var>monitor</var> be the first method argument.
            </li>
            <li>Let <var>track</var> be the <a>MediaStreamTrack</a> object on
            which the method was invoked. (This is the <a>input media stream
            track</a>.)
            </li>
            <li>Associate <var>monitor</var> with <a>input media stream
            track</a> <var>track</var>.
            </li>
          </ol>
          <p>
            The <code><dfn>removeVideoMonitor</dfn>()</code> method, when
            invoked, must run these steps:
          </p>
          <ol>
            <li>Let <var>monitor</var> be the first method argument.
            </li>
            <li>Let <var>track</var> be the <a>MediaStreamTrack</a> object on
            which the method was invoked.
            </li>
            <li>If there exists an association between <var>monitor</var> and
            <var>track</var>, break that association.
            </li>
          </ol>
          <p>
            The <code><dfn>addVideoProcessor</dfn>()</code> method, when
            invoked, must run these steps:
          </p>
          <ol>
            <li>Let <var>processor</var> be the first method argument.
            </li>
            <li>Let <var>track</var> be the <a>MediaStreamTrack</a> object on
            which the method was invoked. (This is the <a>input media stream
            track</a>.)
            </li>
            <li>Associate <var>processor</var> with <a>input media stream
            track</a> <var>track</var>.
            </li>
            <li>Let <var>new track</var> be a newly created
            <a>MediaStreamTrack</a> object. (This is the <a>output media stream
            track</a>.)
            </li>
            <li>Associate <var>new track</var> as the <a>output media stream
            track</a> for <var>processor</var>.
            </li>
            <li>Return <var>new track</var>.
            </li>
          </ol>
          <p>
            The <code><dfn>removeVideoProcessor</dfn>()</code> method, when
            invoked, must run these steps:
          </p>
          <ol>
            <li>Let <var>processor</var> be the first method argument.
            </li>
            <li>Let <var>track</var> be the <a>MediaStreamTrack</a> object on
            which the method was invoked.
            </li>
            <li>If there exists an association between <var>processor</var> and
            <var>track</var>, break that association.
            </li>
          </ol>
        </div>
      </section>
    </section>
    <section id='imagebitmap-extensions'>
      <h2>
        ImageBitmap extensions
      </h2>
      <div class="note">
        <p>
          The <a>ImageBitmap</a> interface is originally designed as a pure
          opaque handler to an image data buffer inside a browser so that how
          the browser stores the buffer is uknown to users and optimized to
          platforms. In this specification, we chooses <a>ImageBitmap</a>
          (instead of <code>ImageData</code>) as the container of video frames
          because the decoded video frame data might exist in either CPU or GPU
          memory which perfectly matches the nature of
          <a><code>ImageBitmap</code></a> as an opaque handler.
        </p>
      </div>
      <p>
        Considering how would developers process video frames, there are two
        possible approaches, via pure JavaScript(/asm.js) code or via WebGL.
      </p>
      <ol>
        <li>If developers use JavaScript(/asm.js) to process the frames, then
        the <a>ImageBitmap</a> interface needs to be extended with APIs for
        developers to access its underlying data and there should also be a way
        for developers to create an <a>ImageBitmap</a> from the processed data.
        </li>
        <li>If developers use WebGL, then WebGL needs to be extended so that
        developers can pass an <a>ImageBitmap</a> into the WebGL context and
        the browser will handle how to upload the raw image data into the GPU
        memory. Possibly, the data is already in the GPU memory so that the
        operation could be very efficient.
        </li>
      </ol>
      <p>
        In this specification, the original <a>ImageBitmap</a> interface is
        extended with three methods to let developers read data from an
        <a>ImageBitmap</a> object into a given <a>BufferSource</a> in a set of
        supported <a>ImageFormat</a>s. How the accessed image data is arranged
        in memory is described by the proposed <a>ImagePixelLayout</a> and
        dictionary <a>ChannelPixelLayout</a>. Also, the
        <a>ImageBitmapFactories</a> interface is extended to let developers
        create an <a>ImageBitmap</a> object from a given <a>BufferSource</a>.
      </p>
      <section id='imageformat'>
        <h2>
          Image format
        </h2>
        <p>
          An image or a video frame is conceptually a two-dimentional array of
          data and each element in the array is called a <dfn>pixel</dfn>. The
          <a data-lt="pixel">pixels</a> are usually stored in a one-dimensional
          array and could be arranged in a variety of <a data-lt=
          "image format">image formats</a>. Developers need to know how the
          <a data-lt="pixel">pixels</a> are formatted so that they are able to
          process them.
        </p>
        <p>
          The <a>image format</a> describes how pixels in an image are
          arranged. A single pixel has at least one, but usually multiple
          <dfn data-lt="pixel value">pixel values</dfn>. The range of a
          <a>pixel value</a> varies, which means different <a data-lt=
          "image format">image formats</a> use different <a data-lt=
          "data type">data types</a> to store a single <a>pixel value</a>.
        </p>
        <p>
          The most frequently used <a>data type</a> is 8-bit unsigned integer
          whose range is from 0 to 255, others could be 16-bit integer or
          32-bit floating points and so forth. The number of <a data-lt=
          "pixel value">pixel values</a> of a single <a>pixel</a> is called the
          <dfn data-lt="number of channel">number of channels</dfn> of the
          <a>image format</a>. Multiple <a data-lt="pixel value">pixel
          values</a> of a <a>pixel</a> are used together to describe the
          captured property which could be color or depth information. For
          example, if the data is a color image in RGB color space, then it is
          a three-channel <a>image format</a> and a <a>pixel</a> is described
          by R, G and B three pixel values with range from 0 to 255. As another
          example, if the data is a gray image, then it is a single-channel
          <a>image format</a> with 8-bit unsigned integer <a>data type</a> and
          the <a>pixel value</a> describes the gray scale. For depth data, it
          is a single channel <a>image format</a> too, but the <a>data type</a>
          is 16-bit unsigned integer and the <a>pixel value</a> is the depth
          level.
        </p>
        <p>
          For those <a data-lt="image format">image formats</a> whose
          <a data-lt="pixel">pixels</a> contain multiple <a data-lt=
          "pixel value">pixel values</a>, the <a data-lt="pixel value">pixel
          values</a> might be arranged in one of the following ways:
        </p>
        <ol>
          <li>
            <dfn>Planar pixel layout</dfn>: each <a>channel</a> has its
            <a data-lt="pixel value">pixel values</a> stored consecutively in
            separated buffers (a.k.a. planes) and then all channel buffers are
            stored consecutively in memory. (Ex:
            RRRRRR......GGGGGG......BBBBBB......)
          </li>
          <li>
            <dfn>Interleaving pixel layout</dfn>: each <a>pixel</a> has its
            <a data-lt="pixel value">pixel values</a> from all <a data-lt=
            "channel">channels</a> stored together and interleaves all
            <a data-lt="channel">channels</a>. (Ex: RGBRGBRGBRGBRGB......)
          </li>
        </ol>
        <div class="note">
          <p>
            <a data-lt="image format">Image formats</a> that belong to the same
            color space might have different <a data-lt="pixel layout">pixel
            layouts</a>.
          </p>
        </div>
        <section id='imageformat-enumaration'>
          <h2>
            <code>ImageFormat</code> enumeration
          </h2>
          <p>
            The <a>ImageFormat</a> enumeration is used to select the <dfn>image
            format</dfn> for the <a>ImagePixelLayout</a>. The
            <a>ImageBitmap</a> extensions defined in this specification use
            this enumeration to negotiate the <a>image format</a> while
            accessing the underlying data of <a>ImageBitmap</a> and creating a
            new <a>ImageBitmap</a>.
          </p>
          <div class="note">
            <p>
              We need to elaborate this list for standardization.
            </p>
          </div>
          <pre class="idl">
            enum ImageFormat {
                "RGBA32",
                "BGRA32",
                "RGB24",
                "BGR24",
                "GRAY8",
                "YUV444P",
                "YUV422P",
                "YUV420P",
                "YUV420SP_NV12",
                "YUV420SP_NV21",
                "HSV",
                "Lab",
                "DEPTH",
                /* empty string */ ""
            };
          </pre>N/A N/A
          <table class="simple">
            <tr>
              <th>
                ImageFormat
              </th>
              <th>
                Channel order
              </th>
              <th>
                Channel size
              </th>
              <th>
                Pixel layout
              </th>
              <th>
                Data type
              </th>
            </tr>
            <tr>
              <td>
                <code><dfn>RGBA32</dfn></code>
              </td>
              <td>
                R, G, B, A
              </td>
              <td>
                full rgba-channels
              </td>
              <td>
                interleaving rgba-channels
              </td>
              <td>
                8-bit unsigned integer
              </td>
            </tr>
            <tr>
              <td>
                <code><dfn>BGRA32</dfn></code>
              </td>
              <td>
                B, G, R, A
              </td>
              <td>
                full bgra-channels
              </td>
              <td>
                interleaving bgra-channels
              </td>
              <td>
                8-bit unsigned integer
              </td>
            </tr>
            <tr>
              <td>
                <code><dfn>RGB24</dfn></code>
              </td>
              <td>
                R, G, B
              </td>
              <td>
                full rgb-channels
              </td>
              <td>
                interleaving rgb-channels
              </td>
              <td>
                8-bit unsigned integer
              </td>
            </tr>
            <tr>
              <td>
                <code><dfn>BGR24</dfn></code>
              </td>
              <td>
                B, G, R
              </td>
              <td>
                full bgr-channels
              </td>
              <td>
                interleaving bgr-channels
              </td>
              <td>
                8-bit unsigned integer
              </td>
            </tr>
            <tr>
              <td>
                <code><dfn>GRAY8</dfn></code>
              </td>
              <td>
                GRAY
              </td>
              <td>
                full gray-channel
              </td>
              <td>
                planar gray-channel
              </td>
              <td>
                8-bit unsigned integer
              </td>
            </tr>
            <tr>
              <td>
                <code><dfn>YUV444P</dfn></code>
              </td>
              <td>
                Y, U, V
              </td>
              <td>
                full yuv-channels
              </td>
              <td>
                planar yuv-channels
              </td>
              <td>
                8-bit unsigned integer
              </td>
            </tr>
            <tr>
              <td>
                <code><dfn>YUV422P</dfn></code>
              </td>
              <td>
                Y, U, V
              </td>
              <td>
                full y-channel, half uv-channels
              </td>
              <td>
                planar yuv-channels
              </td>
              <td>
                8-bit unsigned integer
              </td>
            </tr>
            <tr>
              <td>
                <code><dfn>YUV420P</dfn></code>
              </td>
              <td>
                Y, U, V
              </td>
              <td>
                full y-channel, quarter uv-channels
              </td>
              <td>
                planar yuv-channels
              </td>
              <td>
                8-bit unsigned integer
              </td>
            </tr>
            <tr>
              <td>
                <code><dfn>YUV420SP_NV12</dfn></code>
              </td>
              <td>
                Y, U, V
              </td>
              <td>
                full y-channel, quarter uv-channels
              </td>
              <td>
                planar y-channel, interleaving uv-channels
              </td>
              <td>
                8-bit unsigned integer
              </td>
            </tr>
            <tr>
              <td>
                <code><dfn>YUV420SP_NV21</dfn></code>
              </td>
              <td>
                Y, V, U
              </td>
              <td>
                full y-channel, quarter uv-channels
              </td>
              <td>
                planar y-channel, interleaving vu-channels
              </td>
              <td>
                8-bit unsigned integer
              </td>
            </tr>
            <tr>
              <td>
                <code><dfn>HSV</dfn></code>
              </td>
              <td>
                H, S, V
              </td>
              <td>
                full hsv-channels
              </td>
              <td>
                interleaving hsv-channels
              </td>
              <td>
                32-bit IEEE floating point number
              </td>
            </tr>
            <tr>
              <td>
                <code><dfn>Lab</dfn></code>
              </td>
              <td>
                l, a, b
              </td>
              <td>
                full lab-channels
              </td>
              <td>
                interleaving lab-channels
              </td>
              <td>
                32-bit IEEE floating point number
              </td>
            </tr>
            <tr>
              <td>
                <code><dfn>DEPTH</dfn></code>
              </td>
              <td>
                DEPTH
              </td>
              <td>
                full depth-channel
              </td>
              <td>
                planar depth-channel
              </td>
              <td>
                16-bit unsigned integer
              </td>
            </tr>
            <tr>
              <td>
                <code>""</code> (<dfn>the empty string</dfn>)
              </td>
              <td>
                N/A
              </td>
              <td>
                N/A
              </td>
              <td>
                N/A
              </td>
              <td>
                N/A
              </td>
            </tr>
          </table>
        </section>
        <section id='channelpixellayoutdatatype-enumeration'>
          <h2>
            <code>ChannelPixelLayoutDataType</code> enumeration
          </h2>
          <p>
            The <a>ChannelPixelLayoutDataType</a> enumeration is used to select
            the <dfn>channel data type</dfn> that is used to store a single
            <a>pixel value</a>.
          </p>
          <pre class="idl">
            enum ChannelPixelLayoutDataType {
                "uint8",
                "int8",
                "uint16",
                "int16",
                "uint32",
                "int32",
                "float32",
                "float64"
            };
          </pre>
          <table class="simple">
            <tr>
              <th>
                DataType
              </th>
              <th>
                description
              </th>
            </tr>
            <tr>
              <td>
                <code><dfn>uint8</dfn></code>
              </td>
              <td>
                8-bit unsigned integer.
              </td>
            </tr>
            <tr>
              <td>
                <code><dfn>int8</dfn></code>
              </td>
              <td>
                8-bit integer.
              </td>
            </tr>
            <tr>
              <td>
                <code><dfn>uint16</dfn></code>
              </td>
              <td>
                16-bit unsigned integer.
              </td>
            </tr>
            <tr>
              <td>
                <code><dfn>int16</dfn></code>
              </td>
              <td>
                16-bit integer.
              </td>
            </tr>
            <tr>
              <td>
                <code><dfn>uint32</dfn></code>
              </td>
              <td>
                32-bit unsigned integer.
              </td>
            </tr>
            <tr>
              <td>
                <code><dfn>int32</dfn></code>
              </td>
              <td>
                32-bit integer.
              </td>
            </tr>
            <tr>
              <td>
                <code><dfn>float32</dfn></code>
              </td>
              <td>
                32-bit IEEE floating point number.
              </td>
            </tr>
            <tr>
              <td>
                <code><dfn>float64</dfn></code>
              </td>
              <td>
                64-bit IEEE floating point number.
              </td>
            </tr>
          </table>
        </section>
      </section>
      <section id='pixellayout'>
        <h2>
          Pixel layout
        </h2>
        <p>
          For generalizing the variety of <a data-lt="pixel layout">pixel
          layouts</a> among <a data-lt="image format">image formats</a>, here we
          propose the dictionary <a>ChannelPixelLayout</a> and the
          <a>ImagePixelLayout</a> which is a sequence of
          ChannelPixelLayout.
        </p>
        <p>
          The <a>ImagePixelLayout</a> represents the <dfn>pixel
          layout</dfn> of a certain <a data-lt="image format">image format</a>.
          Since an <a>image format</a> is composed of at least one
          <dfn>channel</dfn>, an <a>ImagePixelLayout</a> object contains
          at least one <a>ChannelPixelLayout</a> object.
          <!-- TODO: more elaborate description of a channel needed -->
        </p>
        <p>
          Although an image or a video frame is a two-dimensional structure,
          its data is usually stored in a one-dimensional array in the
          row-major way and each <a>channel</a> describes how <a data-lt=
          "pixel value">pixel values</a> are arranged in the one dimensional
          array buffer.
        </p>
        <p>
          A <a>channel</a> has an associated <a>offset</a> that denotes the
          beginning position of the <a>channel</a>'s data relative to the given
          <a>BufferSource</a> parameter of the
          <code><a>mapDataInto</a>()</code> method.
        </p>
        <p>
          A <a>channel</a> has an associated <a>width</a> and <a>height</a>
          that denote the width and height of the <a>channel</a> respectively.
          Each <a>channel</a> in an <a>image format</a> may have different
          height and width.
        </p>
        <p>
          A <a>channel</a> has an associated <dfn>data type</dfn> used to store
          one single <a>pixel value</a>.
        </p>
        <p>
          A <a>channel</a> has an associated <a>stride</a> that is the number
          of bytes between the beginning two consecutive rows in memory. (The
          total bytes of each row plus the padding bytes of each raw.)
        </p>
        <p>
          A <a>channel</a> has an associated <a>skip</a> value. The value is
          zero for the <a>planar pixel layout</a>, and a positive integer for
          the <a>interleaving pixel layout</a>. (Describes how many bytes there
          are between two adjacent pixel values in this channel.)
        </p>
        <pre class='example highlight'>
          Example1: RGBA image, width = 620, height = 480, stride = 2560

          chanel_r: offset = 0, width = 620, height = 480, data type = uint8, stride = 2560, skip = 3
          chanel_g: offset = 1, width = 620, height = 480, data type = uint8, stride = 2560, skip = 3
          chanel_b: offset = 2, width = 620, height = 480, data type = uint8, stride = 2560, skip = 3
          chanel_a: offset = 3, width = 620, height = 480, data type = uint8, stride = 2560, skip = 3

                  &lt;---------------------------- stride ----------------------------&gt;
                  &lt;---------------------- width x 4 ----------------------&gt;
          [index] 01234   8   12  16  20  24  28                           2479    2559
                  |||||---|---|---|---|---|---|----------------------------|-------|
          [data]  RGBARGBARGBARGBARGBAR___R___R...                         A%%%%%%%%
          [data]  RGBARGBARGBARGBARGBAR___R___R...                         A%%%%%%%%
          [data]  RGBARGBARGBARGBARGBAR___R___R...                         A%%%%%%%%
                       ^^^
                       r-skip
        </pre>
        <pre class='example highlight'>
          Example2: YUV420P image, width = 620, height = 480, stride = 640

          chanel_y: offset = 0, width = 620, height = 480, stride = 640, skip = 0
          chanel_u: offset = 307200, width = 310, height = 240, data type = uint8, stride = 320, skip = 0
          chanel_v: offset = 384000, width = 310, height = 240, data type = uint8, stride = 320, skip = 0

                  &lt;--------------------------- y-stride ---------------------------&gt;
                  &lt;----------------------- y-width -----------------------&gt;
          [index] 012345                                                  619      639
                  ||||||--------------------------------------------------|--------|
          [data]  YYYYYYYYYYYYYYYYYYYYYYYYYYYYY...                        Y%%%%%%%%%
          [data]  YYYYYYYYYYYYYYYYYYYYYYYYYYYYY...                        Y%%%%%%%%%
          [data]  YYYYYYYYYYYYYYYYYYYYYYYYYYYYY...                        Y%%%%%%%%%
          [data]  ......
                  &lt;-------- u-stride ----------&gt;
                  &lt;----- u-width -----&gt;
          [index] 307200              307509   307519
                  |-------------------|--------|
          [data]  UUUUUUUUUU...       U%%%%%%%%%
          [data]  UUUUUUUUUU...       U%%%%%%%%%
          [data]  UUUUUUUUUU...       U%%%%%%%%%
          [data]  ......
                  &lt;-------- v-stride ----------&gt;
                  &lt;- --- v-width -----&gt;
          [index] 384000              384309   384319
                  |-------------------|--------|
          [data]  VVVVVVVVVV...       V%%%%%%%%%
          [data]  VVVVVVVVVV...       V%%%%%%%%%
          [data]  VVVVVVVVVV...       V%%%%%%%%%
          [data]  ......
        </pre>
        <pre class='example highlight'>
          Example3: YUV420SP_NV12 image, width = 620, height = 480, stride = 640

          chanel_y: offset = 0, width = 620, height = 480, stride = 640, skip = 0
          chanel_u: offset = 307200, width = 310, height = 240, data type = uint8, stride = 640, skip = 1
          chanel_v: offset = 307201, width = 310, height = 240, data type = uint8, stride = 640, skip = 1

                  &lt;--------------------------- y-stride --------------------------&gt;
                  &lt;----------------------- y-width ----------------------&gt;
          [index] 012345                                                 619      639
                  ||||||-------------------------------------------------|--------|
          [data]  YYYYYYYYYYYYYYYYYYYYYYYYYYYYY...                       Y%%%%%%%%%
          [data]  YYYYYYYYYYYYYYYYYYYYYYYYYYYYY...                       Y%%%%%%%%%
          [data]  YYYYYYYYYYYYYYYYYYYYYYYYYYYYY...                       Y%%%%%%%%%
          [data]  ......
                  &lt;--------------------- u-stride / v-stride --------------------&gt;
                  &lt;------------------ u-width + v-width -----------------&gt;
          [index] 307200(u-offset)                                       307819  307839
                  |------------------------------------------------------|-------|
          [index] |307201(v-offset)                                      |307820 |
                  ||-----------------------------------------------------||------|
          [data]  UVUVUVUVUVUVUVUVUVUVUVUVUVUVUV...                      UV%%%%%%%
          [data]  UVUVUVUVUVUVUVUVUVUVUVUVUVUVUV...                      UV%%%%%%%
          [data]  UVUVUVUVUVUVUVUVUVUVUVUVUVUVUV...                      UV%%%%%%%
                   ^            ^
                  u-skip        v-skip
        </pre>
        <pre class='example highlight'>
          Example4: DEPTH image, width = 640, height = 480, stride = 1280

          chanel_d: offset = 0, width = 640, height = 480, data type = uint16, stride = 1280, skip = 0

                  &lt;----------------------- d-stride ----------------------&gt;
                  &lt;----------------------- d-width -----------------------&gt;
          [index] 012345                                                  1280
                  ||||||--------------------------------------------------|
          [data]  DDDDDDDDDDDDDDDDDDDDDDDDDDDDD...                        D
          [data]  DDDDDDDDDDDDDDDDDDDDDDDDDDDDD...                        D
          [data]  DDDDDDDDDDDDDDDDDDDDDDDDDDDDD...                        D
          [data]  ......
        </pre>
        <section id='channelpixellayout-dictionary'>
          <h2>
            <code><dfn>ChannelPixelLayout</dfn></code> dictionary
          </h2>
          <p>
            Each <a>channel</a> is represented by an <a>ChannelPixelLayout</a>
            object.
          </p>
          <pre class="idl">
            dictionary ChannelPixelLayout {
                required unsigned long offset;
                required unsigned long width;
                required unsigned long height;
                required ChannelPixelLayoutDataType dataType;
                required unsigned long stride;
                required unsigned long skip;
            };
          </pre>
          <div>
            <p>
              The <dfn>offset</dfn> attribute represents the <a>channel</a>'s
              <a>offset</a>.
            </p>
            <p>
              The <dfn>width</dfn> attribute represents the <a>width</a> of the
              <a>channel</a>. (Channels in an image format may have different
              width.)
            </p>
            <p>
              The <dfn>height</dfn> attribute represents the <a>height</a> of
              the <a>channel</a>. (Channels in an image format may have
              different height.)
            </p>
            <p>
              The <dfn>dataType</dfn> attribute must return the <a>data
              type</a> of the <a>channel</a>, one of the <a data-lt=
              "channel data type">channel data types</a>.
            </p>
            <p>
              The <dfn>stride</dfn> attribute represents the <a>stride</a> of
              the <a>channel</a>.
            </p>
            <p>
              The <dfn>skip</dfn> attribute represents the <a>skip</a> value
              for the <a>channel</a>.
            </p>
          </div>
        </section>
        <section id='imagepixellayout-definition'>
          <h2>
            <code>ImagePixelLayout</code> definition
          </h2>
          <pre class="idl">
            typedef sequence&lt;ChannelPixelLayout&gt; ImagePixelLayout;
          </pre>
        </section>
      </section>
      <section id='imagebitmap-interface-extensions'>
        <h2>
          <code>ImageBitmap</code> interface
        </h2>
        <pre class="idl">
          [Exposed=(Window,Worker)]
          partial interface ImageBitmap {
              ImageFormat                     findOptimalFormat (optional sequence&lt;ImageFormat&gt; possibleFormats);
              long                            mappedDataLength (ImageFormat format);
              Promise&lt;ImagePixelLayout&gt; mapDataInto (ImageFormat format, BufferSource buffer, long offset, long length);
          };
        </pre>
        <div>
          <p>
            The <code><dfn>findOptimalFormat</dfn>(possibleFormats)</code>
            method must run the following steps:
          </p>
          <ol>
            <li>Let <var>image bitmap</var> be the object on which the method
            was invoked.
            </li>
            <li>Let <var>possible formats</var> be the first argument.
            </li>
            <li>If <var>possible formats</var> is empty, return
            <a>ImageFormat</a> that is <dfn>the most suitable image
            format</dfn> for <var>image bitmap</var>, and terminate these
            steps.
              <div class="note">
                It is up to the implementation to decide how to choose <a>the
                most suitable image format</a> from a list of image formats.
              </div>
            </li>
            <li>If none of the <a data-lt="image format">image formats</a> in
            <var>possible formats</var> is an <a>image format</a> that the user
            agent knows it can render, return <a>the empty string</a> and
            terminate these steps.
            </li>
            <li>Otherwise, return <a>ImageFormat</a> that is <a>the most
            suitable image format</a> out of <var>possible formats</var> for
            <var>image bitmap</var>, and terminate these steps.
            </li>
          </ol>
          <p>
            The <code><dfn>mappedDataLength</dfn>(format)</code> method must
            run the following steps:
          </p>
          <ol>
            <li>Let <var>image bitmap</var> be the object on which the method
            was invoked.
            </li>
            <li>Let <var>format</var> be the first argument.
            </li>
            <li>If the user agent cannot render <var>image bitmap</var>
            represented in <var>format</var>, throw
            <code>NotSupportedError</code>.
            </li>
            <li>Otherwise, return the length of the <a>pixel layout</a> for
            <var>image bitmap</var> represented in <var>format</var> <a>image
            format</a>.
            </li>
          </ol>
          <p>
            The <code><dfn>mapDataInto</dfn>(format, buffer, offset,
            length)</code> method must run the following steps:
          </p>
          <ol>
            <li>Let <var>promise</var> be a new promise.
            </li>
            <li>Run these substeps <a>in parallel</a>:
              <ol>
                <li>Let <var>image bitmap</var> be the object on which the
                method was invoked.
                </li>
                <li>If <var>image bitmap</var> was <a>cropped to the source
                rectangle</a> so that it contains any transparent black pixels
                (cropping area is outside of the source image), then reject
                <var>promise</var> with <code>IndexSizeError</code> and abort
                these steps.
                </li>
                <li>Let <var>format</var>, <var>buffer</var>,
                <var>offset</var>, and <var>length</var> be the similarly named
                method arguments.
                </li>
                <li>If the user agent cannot render <var>image bitmap</var>
                represented in <var>format</var>, reject <var>promise</var>
                with <code>NotSupportedError</code> and terminate these steps.
                </li>
                <li>Make a copy of the underlying image data of <var>image
                bitmap</var> in the given format <var>format</var> into
                <a>BufferSource</a> <var>buffer</var> at offset
                <var>offset</var>, filling at most <var>length</var> bytes.
                </li>
                <li>Let <var>pixel layout</var> be a new
                <a>ImagePixelLayout</a> object that represents the image
                data in the previous step.
                </li>
                <li>Resolve <var>promise</var> with <a>pixel layout</a>.
                </li>
              </ol>
            </li>
            <li>Return <var>promise</var>.
            </li>
          </ol>
        </div>
      </section>
      <section id='imagebitmapfactories-interface-extensions'>
        <h2>
          <code>ImageBitmapFactories</code> interface
        </h2>
        <pre class="idl">
          [NoInterfaceObject, Exposed=(Window,Worker)]
          partial interface ImageBitmapFactories {
              Promise&lt;ImageBitmap&gt; createImageBitmap (BufferSource buffer, long offset, long length, ImageFormat format, ImagePixelLayout layout);
          };
        </pre>
        <div>
          <p>
            The <code><dfn>createImageBitmap</dfn>(buffer, offset, length,
            format, layout)</code> method must run the following steps:
          </p>
          <ol>
            <li>Let <var>buffer</var> be the container for the raw image data.
            </li>
            <li>Let <var>offset</var> be the <a>offset</a> (beginning position)
            of <var>buffer</var>.
            </li>
            <li>Let <var>length</var> be the length of spaces in
            <var>buffer</var> in which the raw image data is placed into.
            </li>
            <li>Let <var>format</var> be the the <a>image format</a> of the raw
            image data placed in <var>buffer</var>.
            </li>
            <li>Let <var>layout</var> be the <a>pixel layout</a> for the raw
            image data, which describes how the data is arranged in
            <var>buffer</var> using <var>format</var>.
            </li>
            <li>If <var>buffer</var> has been <a>neutered</a>, reject
            <var>promise</var> with <code>InvalidStateError</code> and
            terminate these steps.
            </li>
            <li>Let <var>image bitmap</var> be a newly created
            <code>ImageBitmap</code> object.
            </li>
            <li>Set <var>image bitmap</var>'s bitmap data to the image data
            given by the <code>BufferSource</code> <var>buffer</var>.
            </li>
            <li>Return a new <var>promise</var>, but continue running these
            steps <a>in parallel</a>.
            </li>
            <li>Resolve <var>promise</var> with the new <a>ImageBitmap</a>
            object as the value.
            </li>
          </ol>
        </div>
      </section>
    </section>
    <section>
      <h2>
        Examples
      </h2>
      <p>
        This example demonstrates how to hook a worker to a
        <a>MediaStreamTrack</a> as a <a>video processor</a> and use the
        <a href="#imagebitmap-extensions">ImageBitmap extensions</a> from
        within a worker script.
      </p>
      <div class="note">
        A non-worker example should be added too for completeness.
      </div>
      <section>
        <pre class="example">
          &lt;script&gt;
          var processor = new VideoProcessor();
          var inputTrack = mediaStream.getVideoTracks()[0];
          var outputTrack = inputTrack.addVideoProcessor(processor);
          newMediaStream.addTrack(outputTrack);
          var worker = new Worker("processing.js");
          worker.postMessage({aCommand : 'pass_processor', aProcessor: processor}, [processor]);
          &lt;/script&gt;
        </pre>
        <p>
          The worker executes the following <code>processing.js</code> script:
        </p>
        <pre class="example">
          self.onmessage = function(msg) {
            switch (msg.data.aCommand) {
                  case 'pass_processor':
                          bindProcessor(msg.data.aProcessor);
                      break;
                  default:
                      throw 'no aTopic on incoming message to Worker';
              }
          };

          function bindProcessor(processor) {
            processor.onvideoprocessorchange = function(event) {
              // Check if the browser supports YUV format.
              var bitmap = event.inputImageBitmap;
              var yuvFormats = ["YUV444P", "YUV422P", "YUV420P", "YUV420SP_NV12", "YUV420SP_NV21"];
              var bitmapFormat = bitmap.findOptimalFormat(yuvFormats);
              if (bitmapFormat == "") {
                console.log("The browser does not support YUV formats.");
                return;
              }
              // Get the need buffer size to read the image data in YUV format.
              var bitmapBufferLength = bitmap.mappedDataLength(bitmapFormat);

              // Create the buffer for mapping data out.
              var bitmapBuffer = new ArrayBuffer(bitmapBufferLength);
              var bitmapBufferView = new Uint8ClampedArray(bitmapBuffer, 0, bitmapBufferLength);

              // Map the bitmap's data into the buffer created in the previous step.
              var promise = bitmap.mapDataInto(bitmapFormat, bitmapBuffer, 0, bitmapBufferLength);
              promise.then(function(bitmapPixelLayout) {
                // Read out the y-channel properties.
                var ywidth  = bitmapPixelLayout.channels[0].width;
                var yheight = bitmapPixelLayout.channels[0].height;
                var yoffset = bitmapPixelLayout.channels[0].offset;
                var ystride = bitmapPixelLayout.channels[0].stride;
                var yskip   = bitmapPixelLayout.channels[0].skip;   // This should be 0.

                // Initialize the buffer for the result gray image.
                var rgbaBufferLength = ywidth * yheight * 4;
                var rgbaBuffer = new ArrayBuffer(rgbaBufferLength);
                var rgbaBufferView = new Uint8ClampedArray(rgbaBuffer, 0, rgbaBufferLength);

                // Convert YUV to Gray.
                for (var i = 0; i &lt; yheight; ++i) {
                  for (var j = 0; j &lt; ywidth; ++j) {
                    var index = ystride * i + j;
                    var y = parseFloat(bitmapBufferView[yoffset + index]);
                    rgbaBufferView[index * 4 + 0] = y;
                    rgbaBufferView[index * 4 + 1] = y;
                    rgbaBufferView[index * 4 + 2] = y;
                    rgbaBufferView[index * 4 + 3] = 255;
                  }
                }

                // Create a new ImageBitmap from the processed rgbaBuffer and assign to the
                // event.outputImageBitmap.
                var channelR = {offset:0, width:ywidth, height:yheight, dataType:"uint8", stride:ywith * 4, skip:3);
                var channelG = {offset:1, width:ywidth, height:yheight, dataType:"uint8", stride:ywith * 4, skip:3);
                var channelB = {offset:2, width:ywidth, height:yheight, dataType:"uint8", stride:ywith * 4, skip:3);
                var channelA = {offset:3, width:ywidth, height:yheight, dataType:"uint8", stride:ywith * 4, skip:3);
                var layout = [channelR, channelG, channelB, channelA];
                var p = createImageBitmap(rgbaBuffer, 0, rgbaBufferLength, "RGBA32", layout);
                p.then(function(bitmap) {
                  event.outputImageBitmap = bitmap;
                }).catch(function(ex) {
                  console.log("Call createImageBitmap() failed. Error: " + ex);
                });
              },
              function(ex) {
                console.log("Call mapDataInto() failed. Error: " + ex);
              });
            };
          }
        </pre>
      </section>
    </section>
    <section class='appendix'>
      <h2>
        Acknowledgements
      </h2>
      <p>
        Thanks to Robert O'Callahan for his idea of this design.
      </p>
    </section>
  </body>
</html>
