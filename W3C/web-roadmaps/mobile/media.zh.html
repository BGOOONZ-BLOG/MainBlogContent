<!DOCTYPE html>
<html lang="zh">
  <head>
    <meta charset="utf-8">
    <title>媒体</title>
  </head>
  <body>
    <header>
      <h1>媒体</h1>

      <p>这份文档所列出的全部特性是 Web 向一个既服务于生产也服务于消费的综合性多媒体平台演进的出发点。<a href="https://www.w3.org/2011/webtv/">W3C 媒体与娱乐兴趣组</a>所识别出来的那些日益涌现的促进 Web 与媒体世界融合的需求不断推动着相关技术的发展。移动设备在用户的电视体验中也逐渐扮演着越来越重要的作用。当用户在使用手机观看电视节目时，移动设备可以提供第二屏的体验，辅助用户搜索更多信息，或者与电视节目进行互动。</p>
    </header>
    <main>
      <section class="featureset well-deployed">
        <h2 id="well-deployed-technologies">广泛部署的技术</h2>
        <div data-feature="音频/视频播放">
          <p>HTML5 新增的 <code><strong><a data-featureid="video">&lt;video&gt;</a></strong></code> 和 <code><strong><a data-featureid="audio">&lt;audio&gt;</a></strong></code> 标签极大地改善了 Web 对多媒体内容的集成能力。它们允许将音频和视频内容无插件的嵌入到 Web 上，并允许开发者与音视频内容进行更加自由地交互。这些标签使得音频和视频内容成为 Web 上的一等公民，就像 Web 上已经存在了二十多年的图片一样。</p>
        </div>

        <p data-feature="生成媒体内容"><a data-featureid="mse">媒体源扩展</a>可以让开发人员用 JavaScript 缓冲和生成媒体内容，播放内容可以通过媒体源扩展被流化、增强以及补充完整，这就使得Web应用程序开发人员创建能够处理自适应流格式和协议的库。</p>

        <p data-feature="受保护的内容播放">对于那些需要对内容进行特定保护的媒体分发，<a data-featureid="eme">加密媒体扩展</a>使 Web 应用可以渲染基于内容加密模块的加密媒体流。</p>

        <p data-feature="捕捉音频/视频">随着新的HTML5标签允许播放多更多媒体内容，<a data-featureid="html-media-capture">HTML媒体捕捉</a>定义了一个<strong>基于标记的机制</strong>，使用移动设备上常见的附加相机和麦克风来获取捕捉到的多媒体内容。通过<a data-featureid="getusermedia">媒体捕捉与媒体流API</a>可以直接处理<strong>来自摄像机和麦克风的媒体流</strong>。</p>

        <p data-feature="图像和视频的分析和修改"><a data-featureid="html/2dcontext">画布2D上下文</a> API 允许修改图像，这也为<strong>编辑视频</strong>开启了可能，并为 Web 平台带来了多媒体处理的能力。</p>
      </section>
      <section class="featureset in-progress">
        <h2 id="technologies-in-progress">开发中的技术</h2>

        <p data-feature="音频播放">在 <code>&lt;audio&gt;</code> 元素启用的声明性方法之外，<a data-featureid="webaudio">Web 音频 API</a> 提供了一个成熟的音频处理API，包括对音频内容低延迟播放的支持。</p>

        <div data-feature="分布式渲染">
          <p>随着用户拥有的互联设备的增多，这些设备之间协同工作的需求也随之增涨：</p>
          <ul>
            <li><a data-featureid="secondscreen">呈现 API</a> 使网页可以从移动设备打开并控制位于另一个屏幕上的页面，为多屏 Web 应用开辟了道路。</li>
            <li><a data-featureid="remote-playback">远程回放 API</a> 更专注于控制单独设备上的媒体呈现。</li>
            <li><a data-featureid="secondscreen-openscreen">开放屏幕协议</a>是一套允许控制和接收设备以可互操作的方式实现呈现 API 和远程回放 API 的网络协议。</li>
            <li><a data-featureid="picture-in-picture">画中画</a>规范将允许应用在一个其他活动之上都可以看到的单独微型窗口中启动和控制视频的呈现。</li>
            <li><a data-featureid="audio-output">音频输出设备 API</a> 为音频流提供了类似的功能，使 Web 应用能够选择在哪个音频输出设备上播放给定的声音。</li>
          </ul>
        </div>

        <div data-feature="媒体处理能力">
          <p>移动设备具有广泛的解码（和编码）能力。为了改进用户体验，并在可用时利用先进的设备功能，媒体提供方需要知道用户的设备特定的编解码器是否可以解码一个给定的分辨率、码率或帧率。播放是否流畅和省电？HDR和宽色域内容是否可以呈现？<a data-featureid="media-capabilities">媒体能力</a>规范定义了一个暴露这些信息的 API，以便替换掉 HTML 中比较基础且不够明确的 <code>isTypeSupported()</code> 和 <code>canPlayType()</code> 功能。</p>
          <p>媒体提供者同样也需要一个机制来获取用户所感知的播放质量，以便通过自适应流改变传输内容的质量。<a data-featureid="media-playback-quality">媒体播放质量</a>规范最初是媒体源扩展规范的一部分，可以显示出显示或丢弃的帧的数量。</p>
        </div>

        <p data-feature="媒体聚焦">移动设备常常在锁屏或通知区域有快捷方式来处理主应用程序（例如音乐播放器）的音频输出。底层操作系统负责确定哪些应用程序应该具有媒体焦点。<a data-featureid="mediasession">媒体会话</a>规范可以将这些焦点的变化暴露给Web应用。</p>

        <div data-feature="自动播放">
          <p>为了在移动设备上节约流量、内存和电池使用，并防止可能不需要的媒体播放，浏览器已经制定了自动播放策略，并且可能拒绝自动播放媒体内容。<a data-featureid="autoplay">自动播放策略检测</a>规范是一个让应用知道某个媒体元素的自动播放是否会成功的早期提案。</p>
        </div>

        <div data-feature="VR头盔中的渲染">
          <p><a data-featureid="webxr">WebXR 设备 API</a> 规范是一个允许应用通过 JavaScript 访问和控制头戴式显示器，并创建引人注目的虚拟现实（VR）/增强现实（AR）体验的底层 API，这是使360°的视频内容能够在虚拟现实头盔和移动设备中使用的关键。
          </p>
        </div>

        <p data-feature="捕捉音频/视频"><a href="https://www.w3.org/2011/04/webrtc/">Web实时通讯工作组</a>开发了<a data-featureid="recording">记录来自相机和麦克风的媒体流的API</a>，以<a data-featureid="imagecapture">及通过程序访问相机并拍照的API</a>。</p>

        <div data-feature="点对点连接以及音视频流">
          <p><a href="https://www.w3.org/2011/04/webrtc/">Web实时通讯工作组</a>目前还负责的相关工作如下：</p>
          <ul>
            <li>跨设备的<a data-featureid="p2p">点对点通讯</a>，</li>
            <li><a data-featureid="mst-content-hint">内容提示</a>允许Web应用提示媒体内容的类型（例如语音、音乐、电影、屏幕录制等），以便用户代理可以优化编码或处理参数，</li>
            <li><a data-featureid="webrtc-svc">可伸缩视频编码</a>（SVC），允许Web应用配置编码参数以利用SVC（视频流的子集可以从丢弃较大的视频流中的数据包派生而来），从而使从单一初始视频流向多个目标提供不同播放质量的视频更加容易，</li>
            <li>允许用户间实时通讯的<strong>P2P音视频流</strong>。</li>
          </ul>
        </div>
      </section>
      <section class="featureset exploratory-work">
        <h2 id="exploratory-work">探索性工作</h2>

        <div data-feature="分布式渲染">
          <p>多设备时计社区组目前正在探索多设备媒体渲染的另一方面：<a data-featureid="timing">时计对象</a>规范允许在跨设备并独立于网络拓扑的同时，保持视频、音频和其他数据流的同步性。这项努力需要更多有关方面的支持来取得进展。</p>
        </div>

        <div data-feature="不同色彩空间的渲染">
          <p>新的移动屏幕可以通过使用比传统的sRGB色彩空间更广泛的色彩空间提供高分辨率的内容。为了适应宽色域显示，Web的所有图形系统都需要适应这些更广泛的色彩空间。<a data-featureid="css-color-space/icc-colors">CSS 色彩第四版</a> 提议定义在经典sRGB色彩空间之外的CSS色彩。同样，关于 <a data-featureid="color-canvas">canvas 的色彩管理</a>的工作将加强对 HTML <code>canvas</code> 的多色彩支持。</p>
          <p>由<a href="https://www.w3.org/community/colorweb/">Web上的色彩社区组</a>开发的文档——<a data-featureid="colorweb">Web上的高动态范围和宽色域</a>分析了在Web上支持高动态范围（HDR）和宽色域（WCG）的技术差距和可能的下一步计划，例如允许网页中的 HDR 视频内容与周围或覆盖的图形和文本内容之间的色彩和亮度匹配的机制。</p>
        </div>

        <div data-feature="视频处理">
          <p><a data-featureid="web-codecs">WebCodecs</a> 提案提供对内置（软件和硬件）媒体编码器和解码器的高效底层访问，以更好地支持特定的编解码方案，例如点对点音视频会议、低延迟游戏直播或客户端媒体效果和转码，使开发者不必依赖 JavaScript 或 WebAssembly 的编解码器实现，因为这些实现在 CPU、内存、电池和带宽使用方面的成本更高。</p>
          <p>使用 Canvas API 进行视频处理非常耗费 CPU 资源。除了传统的视频处理之外，现代 GPU 通常提供具有直接适用于增强现实应用程序的先进视觉处理能力（例如脸部和物体识别）。<a data-featureid="shape-detection">加速的图形识别</a>正在探索这个空间。</p>
        </div>

        <div data-feature="音频播放">
          <p>即使引入了 <a href="https://www.w3.org/TR/webaudio/#audioworklet">audio worklet</a>，底层音频处理仍然受到 Web 音频 API 的渲染机制的限制。<a data-featureid="audio-device-client">音频设备客户端</a>提案作为 Web 音频 API 和浏览器使用的实际音频设备之间的中间层，可通过可配置参数（如采样率、回调缓冲区大小和通道数）更直接地访问音频硬件，同时可在专用线程中进行处理。</p>
        </div>
      </section>
      <section>
        <h2 id="features-not-covered-by-ongoing-work">目前尚未覆盖的工作</h2>
        <dl>
          <dt>360°视频渲染的原生支持</dt>
          <dd>虽然已经有可能在 <code>&lt;video&gt;</code> 元素中呈现360°视频，但是对360°视频渲染的集成支持将允许隐藏底层自适应流媒体逻辑到应用的复杂性，让浏览器本身能够优化流媒体和渲染。</dd>

          <dd>Canvas API 提供了执行图像和视频处理的功能，但这些功能受限于其对 CPU 执行的依赖性；现代 GPU 为广泛的操作提供硬件加速，但是浏览器不提供这些操作的钩子。GPU for the Web 社区组正在讨论将 GPU 计算功能开放给 Web 应用的解决方案，这可能最终允许 Web 应用利用 GPU 的能力有效地处理视频流。</dd>
        </dl>
      </section>
      <section>
        <h2 id="discontinued-features">不再进行的工作</h2>
        <dl>
          <dt>网络服务发现</dt>
          <dd><a data-featureid="discovery">网络服务发现API</a>通过提供与本地基于网络的媒体渲染器的集成，如 DLN 及 UPnP 等，为建立多设备操作提供较底层的方法。出于对隐私保护的顾虑及缺乏实现，这项工作没有继续下去。目前的解决方案是用户用代理来处理底层的网络发现，就像在<a data-featureid="secondscreen">呈现 API</a> 和<a data-featureid="remote-playback">远程回放 API</a> 中处理的那样。</dd>

          <dt>WebVR</dt>
          <dd>允许访问和控制虚拟现实（VR）设备并且在某些浏览器中已实现的 <a data-featureid="webvr">WebVR</a> 规范的开发已停止，以支持 <a data-featureid="webxr">WebXR 设备 API</a>，该 API 将工作范围扩展到增强现实（AR）设备。</dd>
        </dl>
      </section>
    </main>
    <script src="../js/generate.js"></script>
  </body>
</html>
