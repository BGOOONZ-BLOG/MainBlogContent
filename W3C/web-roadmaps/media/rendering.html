<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>Media Rendering</title>
  </head>
  <body>
    <header>
      <h1>Media Rendering</h1>
      <p>Web browsers are available on billion of devices of different shapes and functions, providing as many targets for delivering media to users.</p>
    </header>
    <main>
      <section class="featureset well-deployed">
        <h2>Well-deployed technologies</h2>
        <div data-feature="Audio/Video rendering">
          <p>Few media-based services can usefully function without rendering audio or video content; the HTML5 specification provides widely deployed support for this essential feature. Video content can be rendered in any Web page via the <a data-featureid="video"><code>&lt;video&gt;</code> element</a>.</p>
          <p>Likewise, audio content can be rendered in any Web page via the <a data-featureid="audio"><code>&lt;audio&gt;</code> element</a>.</p>
          <p>Beyond the declarative approach enabled by the <code>&lt;audio&gt;</code> element, the <a data-featureid="webaudio">Web Audio API</a> provides a full-fledged audio processing API, which includes support for low-latency playback of audio content.</p>
        </div>

        <p data-feature="Rendering of captions"><a data-featureid="webvtt">WebVTT</a> is a file format for captions and subtitles. The specification is still at the Candidate Recommendation phase, but the format is already supported at various levels among browsers, allowing to render text tracks through a <code>&lt;video&gt;</code> element. The <a data-featureid="ttml">Timed Text Markup Language</a> (TTML) specification provides a richer language for describing timed text. Version 2 extends the former standard with advanced features for animations, styling, embedded content and metadata. It is used both as an interchange format among authoring systems and for delivery of subtitles and captions worldwide, in particular through profiles such as the <a data-featureid="ttml-imsc11">IMSC1.1 (Internet Media Subtitles and Captions) profile</a>. Usual browsers do not support IMSC1 natively, but Web applications can still take advantage of IMSC1 through libraries such as the <a href="https://github.com/sandflow/imscJS">imscJS polyfill library</a>, which is a complete implementation of the IMSC1 profile in JavaScript and renders IMSC1 documents to HTML5.</p>
        <p data-feature="Rendering of protected media">For the distribution of media whose content needs specific protection from copy, <a data-featureid="eme">Encrypted Media Extensions</a> (EME) enables Web applications to render encrypted media streams based on Content Decryption Modules (CDM).</p>
        <p data-feature="Rendering of media fragments">Users often want to share a pointer to a specific position within the timeline of a video (or an audio) feed with friends on social networks, and expect media players to jump to the requested position right away. The <a data-featureid="media-frags">Media Fragments URI</a> specification defines a syntax for constructing media fragment URIs and explains how Web browsers can use this information to render the media fragment.</p>
      </section>
      <section class="featureset in-progress">
        <h2>Specifications in progress</h2>
        <div data-feature="Rendering in different color spaces">
          <p>Wide-gamut displays are becoming more and more common. <a data-featureid="css-color-space/icc-colors">CSS Colors Level 4</a> adds color spaces beyond the classical sRGB so that style sheets can leverage new colors available. Similarly, the <a data-featureid="color-gamut">CSS Media Queries Level 4</a> specification includes means to detect wide-gamut displays and adapt the rendering of the application to these improved color spaces.</p>
          <p>The <a data-featureid="mediaqueries5">CSS Media Queries Level 5</a>, to be published after Level 4, will introduce a <code>video-</code> prefix to some features to detect differences between the video plane and the graphics plane on devices that render video separately from the rest of an HTML document, and that e.g. may support High Dynamic Range (HDR) for video and only Standard Dynamic Range (SDR) for other types of content.</p>
        </div>
        <p data-feature="Rendering of protected media">Four extensions to Encrypted Media Extensions (EME) are under development in the Media Working Group: <a data-featureid="eme-v2/persistent-usage-record">Persistent Usage Record sessions</a>, <a data-featureid="eme-v2/hdcp-detection">HDCP Detection</a>, <a data-featureid="eme-v2/encryption-scheme">Encryption scheme capability detection</a> and an <a data-featureid="eme-v2/find-existing-sessions">API to find existing sessions</a>.</p>
        <div data-feature="Rendering of captions">
          <p>The <a data-featureid="adpt">Audio Description profile</a> is a profile of TTML2 incorporating audio features, intended to support audio description workflows worldwide, including description creation, script delivery and exchange and generated audio description distribution.</p>
        </div>
        <div data-feature="Distributed rendering">
          <p>As users increasingly own more and more connected devices, the need to get these devices to work together increases as well:</p>
          <ul>
            <li>The <a data-featureid="secondscreen">Presentation API</a> offers the possibility for a Web page to open and control a page located on another screen, opening the road for multi-screen Web applications.</li>
            <li>The <a data-featureid="remote-playback">Remote Playback API</a> focuses more specifically on controling the rendering of media on a separate device.</li>
            <li>The <a data-featureid="secondscreen-openscreen">Open Screen Protocol</a> is a suite of network protocols that allow user agents to implement the Presentation API and the Remote Playback API in an interoperable fashion for browsers and presentation displays connected via the same local area network.</li>
            <li>The <a data-featureid="picture-in-picture">Picture-in-Picture</a> specification allows applications to initiate and control the rendering of a video in a separate miniature window that is viewable above all other activities.</li>
            <li>The <a data-featureid="audio-output">Audio Output Devices API</a> offers similar functionality for audio streams, enabling a Web application to pick on which audio output devices a given sound should be played on.</li>
          </ul>
        </div>
        <div data-feature="Rendering in VR/AR headsets">
          <p>The <a data-featureid="webxr">WebXR Device API</a> specification is a low-level API that allows applications to access and control head-mounted displays (HMD) using JavaScript and create compelling Virtual Reality (VR) / Augmented Reality (AR) experiences. It is a critical enabler to render 360° video content in Virtual Reality headsets.</p>
        </div>
      </section>
      <section class="featureset exploratory-work">
        <h2>Exploratory work</h2>
        <div data-feature="Audio rendering">
          <p>The <a data-featureid="audio-device-client">Audio Device Client</a> specification proposes to define an intermediate layer between the Web Audio API and actual audio devices used by the browser. It exposes various low-level properties that have been completely hidden or unavailable to developers, including I/O device selection, multi-channel I/O support, and configurable sample rates. The Audio Working Group may adopt the specification as one of its next deliverables.</p>
        </div>
        <div data-feature="Rendering in different color spaces">
          <p>To adapt to wide-gamut displays, all the graphical systems of the Web will need to adapt to these broader color spaces. This includes the need to <a data-featureid="color-canvas">make HTML canvas color-managed</a>.</p>
          <p>More generally, the <a data-featureid="colorweb">High Dynamic Range and Wide Gamut Color on the Web</a> note, developed by the <a href="https://www.w3.org/community/colorweb/">Color on the Web Community Group</a>, analyzes gaps and candidate next steps for enabling support for High Dynamic Range (HDR) and Wide Color Gamut (WCG) on the Web, such as mechanisms to allow color and luminance matching between HDR video content and surrounding or overlaid graphic and textual content in Web pages.</p>
        </div>
        <div data-feature="Rendering of captions">
          <p>Providing an alternative transcript to media content is a well-known best practice; a <a data-featureid="transcript">transcript extension</a> to HTML has been proposed to make an explicit link between media content and their transcript and thus facilitate discovery and consumption.</p>
          <p>Some advanced closed captioning scenarios cannot be expressed using <a data-featureid="webvtt" data-linkonly>WebVTT</a>. In such cases, Web applications need to render cues on their own using JavaScript. By definition, this means that the resulting captions cannot benefit from integration with the underlying platform, e.g. to apply user style sheets or take part in Picture-in-Picture scenarios. <a href="https://w3c.github.io/tpac-breakouts/sessions.html#textcueapi">Early discussions on <code>TextTrackCue</code> enhancements</a> could pave the way for a generic solution in that field.</p>
          <p>Rendering of captions in VR/AR scenarios poses unique challenges on the rendering side (where to position these captions in 3D, whether to follow users head movements, how to indicate the source of a caption in a 360° video when the user is currently looking elsewhere) as well as on the distribution side (how to encode these captions interoperably in timed text files). The Immersive Web Community Group is exploring <a href="https://github.com/immersive-web/proposals/issues/39">Use case for subtitles in 360° videos</a> and <a href="https://github.com/immersive-web/proposals/issues/40">requirements for subtitles and text in WebXR</a>. The <a href="https://www.w3.org/community/immersive-captions/">Immersive Captions Community Group</a> is investigating best practices for access, activation, and display settings for captions with different types of Immersive Media (AR, VR, Games).</p>
          <p>Beyond traditional closed captions, video sharing platforms may implement a feature known as <em>Bullet Chatting</em> or <em>Danmaku</em> whereby comments, which may be generated by users in real-time, and annotations get overlaid and animated on top of videos at specific points of the media timeline. The <a data-featureid="danmaku">Bullet Chatting</a> proposal explores possible interoperability requirements and technical gaps in that space.</p>
        </div>
        <div data-feature="Distributed rendering">
          <p>The Multi-Device Timing Community Group is exploring another aspect of multi-device media rendering: its <a data-featureid="timing">Timing Object</a> specification enables to keep video, audio and other data streams in close synchrony, across devices and independently of the network topology. This effort needs support from interested parties to progress.</p>
        </div>
      </section>
      <section>
        <h2>Features not covered by ongoing work</h2>
        <dl>
          <dt>Native support for 360° video rendering</dt>
          <dd>While it is already possible to render 360° videos within a <code>&lt;video&gt;</code> element, integrated support for the rendering of 360° videos would allow to hide the complexity of the underlying adaptive streaming logic to applications, letting Web browsers optimize streaming and rendering on their own.</dd>

          <dt>Further extensions to Encrypted Media Extensions (EME)</dt>
          <dd>Further extensions to the <a data-featureid="eme">Encrypted Media Extensions</a> specification have been proposed, including defining a virtual environment in which CDMs can run to improve CDM portability across operating systems, mappings between EME and underlying DRM-specific security levels, and protection of media content when played in a VR headset. Development of these features for EME is out of scope for the <a href="https://www.w3.org/media-wg/">Media Working Group</a>. Per <a href="https://www.w3.org/2019/05/media-wg-charter.html">charter</a>, this group can only work on <a data-featureid="eme-v2/persistent-usage-record">Persistent Usage Record sessions</a>, <a data-featureid="eme-v2/hdcp-detection">HDCP Detection</a>, <a data-featureid="eme-v2/encryption-scheme">Encryption scheme capability detection</a> and an <a data-featureid="eme-v2/find-existing-sessions">API to find existing sessions</a>.</dd>
        </dl>
      </section>
      <section>
        <h2>Discontinued features</h2>
        <dl>
          <dt>Network service discovery</dt>
          <dd>The <a data-featureid="discovery">Network Service Discovery API</a> was to offer a lower-level approach to the establishment of multi-device operations, by providing integration with local network-based media renderers, such as those enabled by DLNA, UPnP, etc. This effort was discontinued out of privacy concerns and lack of interest from implementers. The current approach is to let the user agent handle network discovery under the hoods, as done in the <a data-featureid="secondscreen">Presentation API</a> and <a data-featureid="remote-playback">Remote Playback API</a>.</dd>

          <dt>WebVR</dt>
          <dd>Development of the <a data-featureid="webvr">WebVR</a> specification that allowed access and control of Virtual Reality (VR) devices, and which is supported in some browsers, has halted in favor of the <a data-featureid="webxr">WebXR Device API</a>, which extends the scope of the work to Augmented Reality (AR) devices.</dd>
        </dl>
      </section>
    </main>
    <script src="../js/generate.js"></script>
  </body>
</html>
