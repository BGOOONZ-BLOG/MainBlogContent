## WebAppSec WG


### Attendees

* Aaron Shim (Google) *
* Abdulrahman AlQabandi (Microsoft)
* Artur Janc (Google)
* Craig Francis *
* Dan Veditz (co-chair, Mozilla)
* David Dworken (Google) *
* Edward Qiu *
* Francis McCabe (Google)
* Frederik Braun (Mozilla)
* Jack J (Google)
* Jen Ozmen (Google)
* Jerry Zhang (Google) *
* Josh Carlson (Brightcove)
* Krzysztof Kotowicz (Google)
* Lukas Weichselbaum (Google) *
* Mike West (co-chair, Google)
* MikeSmith (W3C/Keio)
* Ryosuke Niwa (Apple) *
* Sam Weiler (W3C/MIT)
* Theresa O'Connor [hober] (Apple)
* Wendy Seltzer (W3C)

[WG participants list](https://www.w3.org/groups/wg/webappsec/participants)


minutes of the previous meeting are at https://github.com/w3c/webappsec/blob/main/meetings/2021/2021-06-15-minutes.md

### Agenda

* Discuss the Formal Objection to including TrustedTypes in the WASWG charter

https://lists.w3.org/Archives/Public/public-new-work/2021Jul/0004.html

https://github.com/w3c/webappsec/issues/595

https://github.com/w3c/webappsec-trusted-types/issues/342

https://auth0.com/blog/securing-spa-with-trusted-types/

Koto's [Trusted Types - mid 2021 report](https://docs.google.com/document/d/1m91JZWKAGOR3jQoicMVE9Ydcq79gM2BetcRIBemrex8/view#)

### Minutes

Mike: Introduced the topic. Pointed to the objection. Asked for Mozilla to comment.

Freddy: We don't see progress in making this more useful for the long tail of the web

Mike: Where's the bar? All websites? Many websites? Some websites?

Freddy: TAG finding noting that specs should be ???. We would like security features that allow more sites to defend against XSS.

Mike: Pointing to the GitHub threads.

Koto: Trusted types is shipping in Chromium browsers, used heavily by Google applications. Added support for a few libraries and frameworks that blocked Google's development, but also bring TT to the broader ecosystem. For example, React, Angular, Lit, DomPurify, [@@ bigger list] etc. support Trusted Types. Written higher-level libraries to make integration simpler. Built tooling to make migration simpler. We've had 0 DOM XSS for applications we've migrated. TT of course doesn't guarantee this, but it incentives writing the application in a way that avoids these issues. VSCode has begun enforcing Trusted Types in their application. Microsoft is using it in Edge's internal pages. Gives another view into whether Trusted Types are deployable/useful. I strongly believe that they do help to increase security of web applications, and harden codebases. Useful for smaller sites as well. Perhaps someone can speak to non-Google experience?

editorial reference: [Jun's blog on the topic](https://microsoftedge.github.io/edgevr/posts/eliminating-xss-with-trusted-types/)

Craig Francis: I've implemented on a few websites. Was actually an easy implementation; simple functions, no-ops to start with, then whittling down the violations.

Freddy: What sites? Legacy or new? Architectural changes?

Craig: Complicated apps for a small buisiness. JavaScript was mostly there to enhance functionality, try to keep that surface small. 100 JS files. Didn't require a lot of changes, but we'd tried to avoid dangerous APIs in the past. We put CSP + TT on in report-only mode, put a very basic policy in, rolled it out. Initially using it early when `TrustedURL` existed. That was harder to deal with. But report-only meant there was no urgency to it, application kept working. Over the course of ~3 months we'd fix things when we had time to spare. Could often just remove old code or rewrite things to use safer methods. Report-only gave us choice in those methods. Still waiting for the sanitizer API, but what we've got right now works.

Sam: I'm not hearing that Trusted Types is "dangerous" or "broken".

Freddy: koto's report is a great summary, good understanding of success and progress you've made. It is probably a very effective runtime enforcement.

Sam: The argument I'm hearing is complexity that's not useful for the long tail.

dveditz: And also, no other implementer has expressed interest. Browser implementers, not website authors, of course.

Sam: Craig, what's your affiliation?

Craig: I've been working with koto informally, just asking him for information.

Mike: [said some things about complexity and implementers]

hober: My understanding of the complexity argument is that the API doesn't provide a sanitizer, just a hook for a sanitizer. So the long tail needs to code to a more complex surface, but also write their own sanitizer or trust a third-party.

Freddy: That's part of our concerns. TT gives a type system, but not the tools to use it. Only guarantees trust that your function was run on the string. Based on Google's safetypes.

Mike: We're building a sanitizer, happily enough. CfC to adopt it into this group, in fact. Feedback welcome.

koto: Lack of a sanitizer is an issue for the web platform. But the complexity argument: we've seen trusted types enforcement, even in report only mode, naturally forces authors to remove violations. In most cases, the code can simply be change to make the code inherently safe by replacing dangerous `innerHTML` with something less dangerous. TT makes the application easier to reason about, easier to audit, and reduces the complexity of this particular aspect of the application. I don't buy the complexity argument, speaking from the experience of deploying TT across many applications.

...: The policy APIs are indeed complicated. We're working through proposals from Jun (MS) to make some behavioral changes that could reduce its complexity without reducing its security. Many other small changes we've made along the way (`TrustedURL`, etc). Recent integrations, folks aren't even coding against TT directly, but against a framework that uses TT internally.

...: I'd be thrilled if we could ship TT without policies. "Perfect Types" that Jun invented enforce TT but don't implement a policy at all, guaranteeing that the dangerous sinks are never called in the application. Gives the ability to create secure-by-default, but we don't believe that that's what applications are ready for today. That would be too much for the long tail.

dveditz: I've not tried to use this through any of the frameworks that have implemented it. Do those frameworks make it easy?

koto: It depends. React, for example, almost never calls the sinks. So a run-of-the-mill React application could probably enforce TT without any code-changes at all. Jun does that in Edge pages. Still need to create TT instances, some frameworks create them safely: Angular sanitizes using its sanitizer, and creates trusted objects itself. React requires you to do more work. We have a library that makes that very simple. Can create `TrustedScriptURL` from a constant, which is very common. Templates. DOMPurify, tell it to give you a `TrustedHTML`, it does the work internally. Sanitizer API in the future could replace that. I think it's straightforward, but I'm biased.

...: VSCode has 1M LOC, only had about 140 violations we had to deal with.

Lukas: Data points on complexity: I've been driving rollouts across Google for CSP and Trusted Types. Important to acknowledge that DOM XSS is a hard problem. TT may not be a simple on-off switch to get rid of it. But it's the simplest solution we have today for this otherwise unsolved problem. We have a lot of deployment experience, it's doable. New applications launch with TT on by default. It's a nice solution. There is a bit of complexity. Some can be tackled by client-side frameworks. That makes it scale to the long-tail. Others with large, legacy applications: to mitigate DOM XSS, you have to get your hands dirty. TT-report-only can show you the parts of your application that should keep you up at night. If you go the next step to enforce it, you get runtime guarantees as well. I'd ask to factor in the difficulty of the problem we want to solve when evaluating the complexity of TT. We've rolled it out to 120+ applications, it's workable. Can also protect the long tail.

MikeSmith: On the queue to suggest that another implementer should talk: Ryosuke?

Ryosuke: One thing I'm wondering about this API: at some point there was a discussion about websites already enforcing this TT-like thing by auditing and having a policy. If that's the case, what's the point of this standard? If we're targeting top-50% of complex websites, and there are tools that do this already, why create the standard? What's the need? From Apple's WebKit team perspective, we don't see this as a high-value mitigation strategy to be adopted. There are other specifications that we're more interested in working on in the near future. There's a distinction between whether some technologies _might_ be useful, vs whether the WG should include it in the charter. Charter means that implemeters are interested in doing the work. It's hard for me to follow the argument that the complexity argument is this, but it might be useful for some site.

Freddy: Previously we had an open charter. "We'd like folks to work on X." That's something we're addressing as part of the charter renewal.

...: Want to push back on the notion of TT being valuable in the context of a React application. When writing a React application, the need for TT is low to non-existant. You write a React application in a way that avoids DOM XSS via React's abstraction on top of the DOM. It's hard (called "DangerouslySetInnerHTML") to use any of the dangerous sinks.

...: Couldn't you indeed polyfill all of TT via JavaScript?

koto: 1. React: I'd be happy if `DangerouslySetInnerHTML` was the only thing we had to consider for DOM XSS. I'd also assume that Google has no DOM XSS, because we had SafeTypes, etc. TT proved that was wrong, was an eye-opener for us in our deployment. Can still have XSS in an application: React is a library: you can use other libraries too. Sites use many. React's support allows it to coexist with those other dependencies. This is a practical problem for Google, surfaced during TT deployment. The browser is the entity that knows which sinks are being used.

...: 2. Polyfill: We started with a polyfill. Several limitations. `javascript:` URLs and report-only mode. No propagation to documents in `<iframe>`. `eval()` can't be replaced. Have to deal with ordering of imports. We did try this.

Wendy: Great conversation. From the W3C team perspective, we think the WebAppSec WG has been a great host to conversations in both incubation and standards track development. Giving this group the room to discuss, with many steps before deciding "This should be a REC."

dveditz: (personal opinion) Chromium implementation exists. It's been useful to other websites as we've heard. So it's not reasonable to think it's going to go away. So the question in my mind is whether it's better to formally standardize with input from others, or let it be a Chrome-only feature. In my personal view, the latter is worse, because others have to polyfill and don't get complete enfrocement. But either way the web benefits from websites recoding their sites to use it and fixing bugs that are uncovered.

MikeSmith: I talked with PLH (director's delegate to resolve formal objection) last week about the handling of this objection. 1. No provision in the policy explicitly for not moving forward based on the argument that it's too complex, or won't be adopted by the long tail. Reasonable argument, but no policy ground to reject from charter. 2. Single-vendor specification, never implemented elsewhere: that's not a reason to reject something from a charter from a process perspective. My personal perspective is that I haven't heard assertions that this is a problematic spec that we can't include. More persuasive would be that this would be bad or dangerous for folks to adopt. But Mozilla categorizes this as "not harmful" in their standards positions. I don't think we have a compelling case to convince the director that this can't/shouldn't be included in the charter.

Sam: Deep appreciation to folks who participated today. Thank you.

