## WebAppSec WG

2020-11-17

### Attendees

* Sam Weiler (W3C/MIT)
* Giorgio Maone
* Wendy Seltzer (W3C)
* Jun Kokatsu (Microsoft)
* Dan Veditz
* Titouan Rigoudy (Google)
* Ian Clelland (Google)
* Camille Lamy (Google)
* Jeff Hodges (Google)
* Frederik Braun (Mozilla)

### Previous Meeting

https://github.com/w3c/webappsec/blob/master/meetings/2020/2020-09-15-minutes.md

### Logistics

* <https://mit.zoom.us/join>
* Meeting ID: 993 7534 3999
* Password: w3c
* US : +1 646 558 8656 or +1 669 900 6833
* International Numbers: https://mit.zoom.us/u/aezpAicYyd

## [Document Policy](https://w3c.github.io/webappsec-permissions-policy/document-policy.html)

ian: last week annevk brought up that document policy is hosted by webappsec but maybe shouldn't be. It was split off permission (formerly feature) policy, which is in this group.

mkwst: we adopted the former Feature Policy, which encompassed both so it could fit, but it feels like it's more incubation in stage. is there any signal from Mozilla?

ian: formally "Non-harmful"

mkwst: then that sounds like incubation

ian: WICG seems like the place

mkwst: Document Policy should be of interest to this group



ian: agenda is just re: home, not substance.

Dan: another time; warning people of it in advance.  

mkwst: could talk about deprecation strategy it might enable.  could fold that in later.



## [CORS-RFC1918](https://wicg.github.io/cors-rfc1918)

titouan: CORS-RFC1918. Websites can embed content from other websites; iframes, videos, etc. Useful! Except, you can embed data from localhost! And private IP addresses, like routers! Seems innocuous enough, but the services deployed on those hosts are often ill-prepared to deal with these requests. Browser currently allows attackers to use your network location to attack things that aren't internet-exposed.

...: Risks: fingerprinting. Detecting local devices is a good way to identify users. Routers: malicious GET requests can hijack DNS settings. Zoom, etc. Slipstream is a recent example. Not a great situation to be in.

...: Requests themselves are the problem. Don't merely want to protect the response data, but mitigate CSRF. Opaque responses aren't enough.

...: We want to automate this. Don't want users to have to choose whether a site should have access to the local network. We also want requests to fail if the local server doesn't explicitly open itself to the web at large. The small number of servers that expect to talk to the internet should carry the burden.

...: CORS has a preflight concept. We can reuse that for this network-based decision. We'll send a preflight when the internet tries to make a request to the intranet, and if it responds explicitly, we'll allow the request.

...: Failure to deal with the preflight leads to failure. Safe.

...: CORS gives origin-level granularity in the decision as to whether or not to allow a given request.

...: HTTP websites can pretend to be any domain, anything on the network. We therefore require requests to devices on a private network to be initiated from a secure context. Easily seprable from the rest of the proposal, easy short-term win.

...: Simple-enough requests are safelisted in CORS. Here, we don't want that. Want to block even simple requests. We also want to block navigations; CSRF risks accrue there as well.

...: We determine which requests to block based on IP addresses. Look at address, classify it as "public", "private", and "local". All of this is under discussion, but likely able to use IANA's special-use subnet registry.

sam: Clarify HTTP?

titouan: Anything on the network can pretend to be a non-secure page.

sam: Do we need to do this at all? What about just blocking these requests.

dan: User agent needs to be able to access resources at these addresses. Navigate to the router, and poke at your configuration. It may not be such an issue for literal RFC1918, but there are internal resources with normal domain names.

titouan: Outside of the spec, we'll likely cover some things with a configuration surface for admins that allows them so specify certain domains as "internal".

mkwst: there are existing services that depend on this. Common example is Plex, which talks to a device on your network to show you TV. Wizards to set up devices, where the support site talks to the device on your network.

Sam: the site that wants to talk to the device could have a meeting point. The device could "phone home" for example.

mkwst: that is how Plex works -- the device registers itself to get a cert. 

dan: Fingerprinting. Browserleak-type proofs-of-content where people scan 192.168.0.* to find hosts on local network, and poke at the servers to determine characteristics.

 * Titouan: https://nullsweep.com/why-is-this-website-port-scanning-me/
 
freddy: UseCounters in Chromium?

titouan: What a segue! We'd obviously like to find out what we're going to break before we launch it? We've instrumented Chromium, waiting for Chrome 87 to roll out to get measurements. We have ideas around things that might break, but would like to confirm that there's not much more. Waiting for measurements right now to see how fast we can roll things out. We're obviously breaking malicious use cases. Also legit cases: Plex is the poster child for this. Went through the trouble of setting up certs, distributing them, etc. so that `https://plex.tv` can talk to the local server over HTTPS. We expect others to be doing the same, no evidence yet.

...: Plex is interesting in particular. We don't want to break them. But. CORS-RFC1918 + Mixed Content Blocking + Rebinding defenses = sadness. A surprisingly large number of routers will block DNS resolutions to private IP addresses. This breaks Plex, as the name given to the local device will no longer work. They fall back to the raw IP address today. This means they can't use the cert, and need to fall back to HTTP. This means that Mixed Content steps in to block the request. This means they fall back to `http://plex.tv`, which will break if we ship CORS-RFC1918.

...: It so happens that their local server also hosts a subset of the functionality on `plex.tv`. This might be a workaround, though not fully-featured.

...: We say "Use secure contexts to talk to local devices." but that's a tall order. Plex did the work. Other IoT folks haven't.

dan: CABForum prevents CAs from issuing certs for non-global names.

titouan: Probably means we need to revisit old proposals to establish TLS connections to IoT devices and things on local networks generally. This is a long-term goal, but seems important.

...: That's where we are in Chromium, waiting for metrics.

dan: This includes `localhost`? Have you worked with things like Zoom, hosted on local device?

titouan: Not yet. Interested in finding these.

dan: Spotify. Dropbox. Many have shifted to protocol handlers.

titouan: If you have feedback, please file an [issue](https://github.com/WICG/cors-rfc1918/issues) or shoot me an email at titouan@google.com.

## Site-Isolation


## Future path for Web Application Security WG


mkwst: I see the short term problems we have on the web wrapped up in site isolation. we've given devs good tools to address injection attacks. CSP, trusted types. Good story for devs. 

... less clear for side channel attacks. XSleaks, SPECTRE. Critically important that we give devs the ability to more completely isolate themselves from the rest of the web. Some proposals a wahile back: estark, joel weinberger, tanvi, mkwst: [ISOLATE ME](https://wicg.github.io/isolation/) proposal. It would be interesting for us to explore. Isolated app less likely to expose itself to application-level defects int he platform. 

... We've build some of the primitives we need COOP, COEP. Should look at further
SPECTRE is not solved. We've shipped some primiitives in FF and chromium that we think improve from status quo. Starting to get a chance to keep data out of processes that shouldn't have access. 
Origin-level isolation. To have effect, likely going to want to revisit default behaviors and older APIs
Suggest we prioritize those that make it difficult to move to origin isolation on process level, things like document.domain
a number of things we ought to do by default. shift the web over time
don't think we can do everything tomorrow without substantial breakage. but things we can push the web towards
would like to explore together changing the defaults. 
Ian, perhaps you can talk about deprecation strategy. 

iclelland: 3/4-baked idea.document policy try to find ways to ratchet down usage. to reduce the incentives for people to increase usage. 

... introduce a configuration point in Document Policy to add a header to remove the ability to use document.domain in your site. if you turn it off origin-wide, we can isolate you. 
at a later point, once usage is low enough, we can flip the default  So document.domain is restricted on most of the web and use document policy to turn it on only when needed. protect all the people who woudlnt take action on their own but don't need it, and you're left with just the sites who require the feature and need more time to move away. Then look for new primitives to help them. 
Eventually you get down to microscopic usage levels, until you can remove the feature altogether. 

[pdfs on the topic: https://secweb.work/presentations/janc2020places.talk.pdf and https://secweb.work/papeemrs/janc2020places.pdfâ€©]

mkwst: I think it's possible to deprecate things. We've done so in the past, but it's hard, especially when there's actual usage. Many things have substanital usage on teh web 0.4-0.5% page usege for document.domain. 

... opt-in strategy that Ian mentioned gives a place for people who can update to move. 

... Consider flash, which has been an incredibly long-term deprecation, but think it has been useful. 


... attackers get better at using the APIs we expose to them all th time. Dealing with these attacks as one-offs won't be effective. Need to match origins to processes (a meaningful object to your computer). Should _embedding_ happen by default? Instead of X-Frame-Options "don't frame me, bro", reverse it and make embedded content declare they intend to be embedded.

Jun: I'm worried about mobile platforms where site isolation isn't shipped even in Chrome. If the site has COOP and COEP they still will not be isolated because there are embedded ads (necessary to the web ecosystem currently). That will allow leaks from ads to the main page or other content in the same tab. Can we use iframe sandbox as a process isolation cue on mobile?

mkwst: we must assume we can have at least two renderer processes -- one for the top level page and one for everything else. inadequate, but might be the bare minimum. There's a huge incentive for ads to allow themselves to be embedded broadly. We could change it so a site has to explicitly opt-in to be embedded by a specific site, which creates some accountability. But yes, you are correct if all this content lies in the same process there is not a lot we can do.

Wendy: at the high level we're talking about ecosystem security, dealing with lots of built systems with funny historical decisions. Will require feedback and input and buy-in from lots of stakeholders. We're needing their input to help us move forward to make the web more secure without destroying what currently works.

Jun: Mike said earlier that Trusted Types will help, but we don't have any primitives to sanitize HTML. As we see DOM Purify bypasses these days it's important that we supply an everygreen sanitizer built-in.

Freddy: we're working on a Sanitizer API that can turn HTML input into markup that will not cause XSS. it's in incubation. https://github.com/WICG/sanitizer-api/
... : can be tested in Firefox Nightly (about:config -> dom.security.sanitizer.enabled)

## Miscellaneous mixed-content news
Dveditz: HTTPS-only preference now in FF latest
